{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOQn5tWCPUsfT0N/YZPq/VW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/monimoreno2905/SegundoParcial/blob/main/GP_Mnist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este código contiene la implementación de un clasificador GP multiclase para m-nist. Así mismo se implementa un clasificador por autoencoder variacional para comparar las ventajas y desventajas de cada método"
      ],
      "metadata": {
        "id": "B03IHBjRYMZl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. INSTALACIÓN Y DESCARGA DE LIBRERIAS"
      ],
      "metadata": {
        "id": "cf9fm3IfalXs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gpflow --upgrade #tensorflow~=2.12.0 tensorflow-probability~=0.20.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yw_CCrvZNbqf",
        "outputId": "d9f693d2-c1a5-449d-e44b-b029bd840bb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gpflow\n",
            "  Downloading gpflow-2.9.2-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting check-shapes>=1.0.0 (from gpflow)\n",
            "  Downloading check_shapes-1.1.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting deprecated (from gpflow)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: multipledispatch>=0.6 in /usr/local/lib/python3.10/dist-packages (from gpflow) (1.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from gpflow) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gpflow) (24.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from gpflow) (1.13.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from gpflow) (71.0.4)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from gpflow) (0.9.0)\n",
            "Requirement already satisfied: tensorflow-probability>=0.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability[tf]>=0.12.0->gpflow) (0.24.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from gpflow) (4.12.2)\n",
            "Requirement already satisfied: tensorflow>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from gpflow) (2.17.0)\n",
            "Collecting dropstackframe>=0.1.0 (from check-shapes>=1.0.0->gpflow)\n",
            "  Downloading dropstackframe-0.1.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting lark<2.0.0,>=1.1.0 (from check-shapes>=1.0.0->gpflow)\n",
            "  Downloading lark-1.2.2-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.4.0->gpflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.4.0->gpflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.4.0->gpflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.4.0->gpflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.4.0->gpflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.4.0->gpflow) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.4.0->gpflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.4.0->gpflow) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.4.0->gpflow) (3.3.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.4.0->gpflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.4.0->gpflow) (2.32.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.4.0->gpflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.4.0->gpflow) (2.4.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.4.0->gpflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.4.0->gpflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.4.0->gpflow) (2.17.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.4.0->gpflow) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.4.0->gpflow) (0.37.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability>=0.12.0->tensorflow-probability[tf]>=0.12.0->gpflow) (4.4.2)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability>=0.12.0->tensorflow-probability[tf]>=0.12.0->gpflow) (2.2.1)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability>=0.12.0->tensorflow-probability[tf]>=0.12.0->gpflow) (0.1.8)\n",
            "Requirement already satisfied: tf-keras>=2.16 in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability[tf]>=0.12.0->gpflow) (2.17.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow>=2.4.0->gpflow) (0.44.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow>=2.4.0->gpflow) (13.8.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow>=2.4.0->gpflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow>=2.4.0->gpflow) (0.12.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow>=2.4.0->gpflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow>=2.4.0->gpflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow>=2.4.0->gpflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow>=2.4.0->gpflow) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow>=2.4.0->gpflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow>=2.4.0->gpflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow>=2.4.0->gpflow) (3.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow>=2.4.0->gpflow) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow>=2.4.0->gpflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow>=2.4.0->gpflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow>=2.4.0->gpflow) (0.1.2)\n",
            "Downloading gpflow-2.9.2-py3-none-any.whl (392 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m392.9/392.9 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading check_shapes-1.1.1-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading dropstackframe-0.1.1-py3-none-any.whl (4.6 kB)\n",
            "Downloading lark-1.2.2-py3-none-any.whl (111 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.0/111.0 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lark, dropstackframe, deprecated, check-shapes, gpflow\n",
            "Successfully installed check-shapes-1.1.1 deprecated-1.2.14 dropstackframe-0.1.1 gpflow-2.9.2 lark-1.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XJwi8WNkNHcX"
      },
      "outputs": [],
      "source": [
        "#Implementación de Keras dentro de tensorflow, para la definición de redes\n",
        "from tensorflow import keras\n",
        "#Permite especificar que un parámetro o retorno puede ser cualquier tipo de secuencia\n",
        "from typing import Sequence\n",
        "#Libreria para graficar\n",
        "import matplotlib.pyplot as plt\n",
        "#Libreria para algunas operaciones matemáticas\n",
        "import numpy as np\n",
        "#Libreria para utilizar lo relacionado a procesos gausianos\n",
        "import gpflow\n",
        "#Ignorar advertencias de tensorflow\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")  # ignore DeprecationWarnings from tensorflow\n",
        "#Libreria para redes neuronales\n",
        "import tensorflow as tf\n",
        "#Libreria para disminuir hyperparametros sin afectar el rendimiento de las pruebas\n",
        "from gpflow.ci_utils import reduce_in_tests\n",
        "#Librerias para imprimir un resumen de los procesos gausianos y controlar si los parámetros son entrenables o no\n",
        "from gpflow.utilities import print_summary, set_trainable\n",
        "#Graficas\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "# reproducibility:\n",
        "np.random.seed(0)\n",
        "tf.random.set_seed(123)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. CARGAR LA BASE DE DATOS"
      ],
      "metadata": {
        "id": "nkydNgbsXPPc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
        "X_train_full = X_train_full.astype(np.float64) / 255 #normalización de los datos\n",
        "X_test = X_test.astype(np.float64) / 255\n",
        "X_train, X_valid = X_train_full[:-50000], X_train_full[-50000:] #Se toman solo algunos datos, no todos\n",
        "y_train, y_valid = y_train_full[:-50000], y_train_full[-50000:]"
      ],
      "metadata": {
        "id": "K6cGNfeLNOpm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cd010dd-0b71-4784-e76a-30a138438b9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "\u001b[1m29515/29515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "\u001b[1m26421880/26421880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "\u001b[1m5148/5148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "\u001b[1m4422102/4422102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Se reajusta las imágenes para que se organicen como una matriz de NxP donde N es la\n",
        "#cantidad total de imágenes y P es las dimensiones (28x28=784 para m-nist)\n",
        "X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
        "y_train=y_train.astype(np.float64)\n",
        "#Matriz de datos y etiquetas que alimenta al clasificador\n",
        "data = (X_train_flat, y_train)"
      ],
      "metadata": {
        "id": "MBzPSRk-ON6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. DEFINICIÓN DEL MODELO GAUSSIANO"
      ],
      "metadata": {
        "id": "hPmnP4g4XQx7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.1. Parametros del modelo\n",
        "\n",
        "**Kernel matern32**\n",
        "\n",
        "$k(x, x') = \\sigma^2 \\left(1 + \\frac{\\sqrt{3} r}{\\ell}\\right) \\exp\\left(-\\frac{\\sqrt{3} r}{\\ell}\\right)$\n",
        "\n",
        "Donde\n",
        "\n",
        "$sigma^2$ es la varianza del kernel.\n",
        "\n",
        "$\\ell$ es la longitud de escala.\n",
        "\n",
        "$ r = \\|x - x'\\| $ es la distancia euclidiana entre los puntos \\(x\\) y \\(x'\\)\n",
        "\n",
        "**White kernel**\n",
        "\n",
        "$k(x, x') = \\sigma^2 \\delta(x, x')$\n",
        "\n",
        "Donde\n",
        "\n",
        "$\\sigma^2$ es la varianza del ruido\n",
        "\n",
        " $\\delta(x, x')$ es la función delta de Dirac"
      ],
      "metadata": {
        "id": "BStRPu26gOUk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gpflow.kernels import SquaredExponential, ArcCosine\n",
        "\n",
        "C=10 #Número de clases\n",
        "\n",
        "\"\"\"Definición del Kernel\n",
        "El kernel matern logra modelar funciones suaves pero con cierta flexibilidad para ajustarse a datos que pueden tener irregularidades\n",
        "El kernel white introduce ruido blanco en el modelo, lo cual es útil para modelar el ruido en los datos y evitar sobreajuste\n",
        "\"\"\"\n",
        "#kernel = gpflow.kernels.SquaredExponential() + gpflow.kernels. ArcCosine(order=1)\n",
        "kernel = gpflow.kernels.Matern32() + gpflow.kernels.White(variance=0.01)\n",
        "\n",
        "\"\"\"Robustmax Inverse Multiclass Likelihood\n",
        "La funciónlink transforma las predicciones del modelo dentro de una escala apropiada para la función probabilidad como por ejemplo sigmoide\n",
        "La función invlink hace lo opuesto, es decir, convierte la escala de probabilidad en la escala original de las salidas del modelo\n",
        "Lo anterior se hace debido a que likelihood no es gaussiana\n",
        "\"\"\"\n",
        "invlink = gpflow.likelihoods.RobustMax(C)  # Robustmax inverse link function\n",
        "\n",
        "\"\"\"Multiclass Likelihood\n",
        "Dentro de esta función, invilink se utiliza para mapear las salidas del modelo a una función de densidad acumulativa que pueda darme\n",
        "los valores de probabilidad requeridos para la clasificación teniendo en cuenta que hay 10 clases.\n",
        "Lo anterior se hace porque la salida no se modela directamente sino que se modela una función f con un proceso gausiano que luego\n",
        "se pasa a traves de una función sigmoide o de densidad de probabilidad para obtener las probabilidades de pertenencia a cada clase.\n",
        "\"\"\"\n",
        "likelihood = gpflow.likelihoods.MultiClass(\n",
        "    C, invlink=invlink\n",
        ")\n",
        "\n",
        "\"\"\"Variables de inducción\n",
        "Se escogen algunos puntos del conjunto total de datos. Estos puntos se mantendran fijos durante el entrenamiento del modelo\n",
        "y cuando ingrese un nuevo dato para predecir, se comparará las distribuciones del nuevo punto con las variables de inducción\n",
        "para saber a que clase pertenece.\n",
        "\"\"\"\n",
        "Z = X_train_flat[::50].copy()\n",
        "\n",
        "\n",
        "\"\"\"Modelo sparse variational gaussian process\n",
        "Se ingresa el kernel, la probabilidad, los datos, las clases\n",
        "El parametro whiten facilita la optimización de los parametros asegurando que la matriz de covarianza sea diagonal\n",
        "El parametro q_diag asume una matriz de covarianza diagonal para la variación del modelo posterior disminuyendo costo computacional\n",
        "\"\"\"\n",
        "m = gpflow.models.SVGP(\n",
        "    kernel=kernel,\n",
        "    likelihood=likelihood,\n",
        "    inducing_variable=Z,\n",
        "    num_latent_gps=C,\n",
        "    whiten=True,\n",
        "    q_diag=True,\n",
        ")\n",
        "\n",
        "#Establecer que parametros no entrenar\n",
        "set_trainable(m.kernel.kernels[1].variance, False) #La varianza del segundo kernel no seria ajustada durante el entrenamiento\n",
        "set_trainable(m.inducing_variable, False) #Evitar que el modelo ajuste los puntos del espacio latente\n",
        "print_summary(m, fmt=\"notebook\") #Resumen del modelo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        },
        "id": "wGmsaBgxOXDU",
        "outputId": "68684544-c6dc-450f-88c6-0923011ae141"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table>\n",
              "<thead>\n",
              "<tr><th>name                               </th><th>class    </th><th>transform  </th><th>prior  </th><th>trainable  </th><th>shape     </th><th>dtype  </th><th>value                </th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>SVGP.kernel.kernels[0].variance    </td><td>Parameter</td><td>Softplus   </td><td>       </td><td>True       </td><td>()        </td><td>float64</td><td>1.0                  </td></tr>\n",
              "<tr><td>SVGP.kernel.kernels[0].lengthscales</td><td>Parameter</td><td>Softplus   </td><td>       </td><td>True       </td><td>()        </td><td>float64</td><td>1.0                  </td></tr>\n",
              "<tr><td>SVGP.kernel.kernels[1].variance    </td><td>Parameter</td><td>Softplus   </td><td>       </td><td>False      </td><td>()        </td><td>float64</td><td>0.009999999999999998 </td></tr>\n",
              "<tr><td>SVGP.likelihood.invlink.epsilon    </td><td>Parameter</td><td>Sigmoid    </td><td>Beta   </td><td>False      </td><td>()        </td><td>float64</td><td>0.0010000000000000005</td></tr>\n",
              "<tr><td>SVGP.inducing_variable.Z           </td><td>Parameter</td><td>Identity   </td><td>       </td><td>False      </td><td>(200, 784)</td><td>float64</td><td>[[0., 0., 0....      </td></tr>\n",
              "<tr><td>SVGP.q_mu                          </td><td>Parameter</td><td>Identity   </td><td>       </td><td>True       </td><td>(200, 10) </td><td>float64</td><td>[[0., 0., 0....      </td></tr>\n",
              "<tr><td>SVGP.q_sqrt                        </td><td>Parameter</td><td>Softplus   </td><td>       </td><td>True       </td><td>(200, 10) </td><td>float64</td><td>[[1., 1., 1....      </td></tr>\n",
              "</tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.2. Entrenamiento del modelo\n",
        "\n",
        "**Ecuación Optmizador**\n",
        "\n",
        "$\n",
        "\\theta_{k+1} = \\theta_k - H_k^{-1} \\nabla f(\\theta_k)$\n",
        "\n",
        "\n",
        "Donde:\n",
        "\n",
        "$ \\theta_k $ son los parámetros en la iteración k,\n",
        "\n",
        "$ H_k^{-1}$ es una aproximación de la inversa de la matriz Hessiana de la función objetivo (f),\n",
        "\n",
        "$\\nabla f(\\theta_k)$ es el gradiente de la función objetivo en $\\theta_k$.\n",
        "\n",
        "**Ecuación Función de perdida**\n",
        "\n",
        "\n",
        "$\\mathcal{L}(\\mathbf{f}) = \\mathbb{E}_{q(\\mathbf{f})}[\\log p(\\mathbf{y} \\mid \\mathbf{f})] - \\text{KL}(q(\\mathbf{f}) \\| p(\\mathbf{f}))$\n",
        "\n",
        "\n",
        "Donde:\n",
        "\n",
        " $\\mathcal{L}(\\mathbf{f})$  es el límite inferior variacional,\n",
        "\n",
        "$\\mathbb{E}_{q(\\mathbf{f})}[\\log p(\\mathbf{y} \\mid \\mathbf{f})]$ es la expectativa de la log-verosimilitud de los datos dados las funciones latentes bajo la distribución variacional,\n",
        "\n",
        "$\\text{KL}(q(\\mathbf{f}) \\| p(\\mathbf{f}))$ es la divergencia de Kullback-Leibler entre la distribución variacional $q(\\mathbf{f})$ y la distribución a priori $p(\\mathbf{f})$.\n",
        "\n"
      ],
      "metadata": {
        "id": "dcXjUmpUXUyL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Optimizador\n",
        "Por defecto se está utilizando el optimizador L-BFGS-B (Limited-memory Broyden-Fletcher-Goldfarb-Shanno with Box constraints)\n",
        "El optimizador utiliza aproximaciones a la matriz Hessiana (segunda derivada de la función de pérdida) para realizar actualizaciones de los parámetros\n",
        "\"\"\"\n",
        "opt = gpflow.optimizers.Scipy()\n",
        "\n",
        "\"\"\"Se optimiza con la función de perdida Variational Lower Bound\n",
        "que es el límite inferior de la log-marginal likelihood \"\"\"\n",
        "opt_logs = opt.minimize(\n",
        "    m.training_loss_closure(data),\n",
        "    m.trainable_variables, #se le indica al optimizador que ajuste los parámetros entrenables\n",
        "    options=dict(maxiter=reduce_in_tests(1000)), # define el número máximo de iteraciones del optimizador\n",
        ")\n",
        "\n",
        "print_summary(m, fmt=\"notebook\") #Se imprime resumen del modelo ya entrenado"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        },
        "id": "50qKc7qUQcQ8",
        "outputId": "fd86d54e-2158-4c15-e36d-39406567f9c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table>\n",
              "<thead>\n",
              "<tr><th>name                               </th><th>class    </th><th>transform  </th><th>prior  </th><th>trainable  </th><th>shape     </th><th>dtype  </th><th>value                                     </th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>SVGP.kernel.kernels[0].variance    </td><td>Parameter</td><td>Softplus   </td><td>       </td><td>True       </td><td>()        </td><td>float64</td><td>56.63459                                  </td></tr>\n",
              "<tr><td>SVGP.kernel.kernels[0].lengthscales</td><td>Parameter</td><td>Softplus   </td><td>       </td><td>True       </td><td>()        </td><td>float64</td><td>80.71305                                  </td></tr>\n",
              "<tr><td>SVGP.kernel.kernels[1].variance    </td><td>Parameter</td><td>Softplus   </td><td>       </td><td>False      </td><td>()        </td><td>float64</td><td>0.009999999999999998                      </td></tr>\n",
              "<tr><td>SVGP.likelihood.invlink.epsilon    </td><td>Parameter</td><td>Sigmoid    </td><td>Beta   </td><td>False      </td><td>()        </td><td>float64</td><td>0.0010000000000000005                     </td></tr>\n",
              "<tr><td>SVGP.inducing_variable.Z           </td><td>Parameter</td><td>Identity   </td><td>       </td><td>False      </td><td>(200, 784)</td><td>float64</td><td>[[0., 0., 0....                           </td></tr>\n",
              "<tr><td>SVGP.q_mu                          </td><td>Parameter</td><td>Identity   </td><td>       </td><td>True       </td><td>(200, 10) </td><td>float64</td><td>[[-0.81315245, -0.67642011, -0.32684294...</td></tr>\n",
              "<tr><td>SVGP.q_sqrt                        </td><td>Parameter</td><td>Softplus   </td><td>       </td><td>True       </td><td>(200, 10) </td><td>float64</td><td>[[0.00339651, 0.00619568, 0.0030468...    </td></tr>\n",
              "</tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.3. Medidas de eficiencia\n",
        "\n",
        "$\\text{Exactitud} = \\frac{\\text{TP} + \\text{TN}}{\\text{TP} + \\text{TN} + \\text{FP} + \\text{FN}}$\n",
        "\n",
        "$\\text{Precisión} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}}$\n",
        "\n",
        "$\\text{Recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}$\n",
        "\n",
        "$\\text{F1 Score} = 2 \\times \\frac{\\text{Precisión} \\times \\text{Recall}}{\\text{Precisión} + \\text{Recall}}$\n",
        "\n",
        "Donde\n",
        "\n",
        "Tp son los verdaderos positivos\n",
        "\n",
        "TN son los verdaderos negativos\n",
        "\n",
        "Fp son los verdaderos positivos\n",
        "\n",
        "FN son los verdaderos negativos"
      ],
      "metadata": {
        "id": "TBSdUgahXX2D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Primero se hacen predicciones sobre los datos totales\"\"\"\n",
        "Fmean, _ = m.predict_f(X_train_flat) #Se hace predicciones sobre el conjunto total de datos\n",
        "P = m.likelihood.invlink(Fmean) #Se aplica la invilink para ajustar las probabilidades de clasificación\n",
        "array = P.numpy() # Convierte el tensor a un array numpy\n",
        "max_positions = np.argmax(array, axis=1) #Encuentra la clase a la que pertenece cada elemento según las probabilidades"
      ],
      "metadata": {
        "id": "jNxXJ8pirLZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "y_true = y_train\n",
        "y_pred = max_positions\n",
        "\n",
        "#Calcular accuracy (porcentaje total de aciertos)\n",
        "accuracy = np.mean(y_train == y_pred)\n",
        "\n",
        "# Calcular precisión (mide la exactitud de las predicciones positivas de un modelo)\n",
        "precision = precision_score(y_true, y_pred, average='weighted')\n",
        "\n",
        "# Calcular recall (mide la capacidad de un modelo para identificar todos los casos positivos)\n",
        "recall = recall_score(y_true, y_pred, average='weighted')\n",
        "\n",
        "# Calcular F1 score (combina la precisión y el recall en una sola medida.)\n",
        "f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "\n",
        "#Calcular matriz de confusion\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "# Imprimir resultados\n",
        "print(cm)\n",
        "print(\"Precisión:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFhb8sMJt0hX",
        "outputId": "2c33a27a-dd8c-4f7b-cdcd-481b2c39e363"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[829   1  12  33   5   0  54   0   8   0]\n",
            " [  5 995   6  19   1   0   1   0   0   0]\n",
            " [ 10   2 822   8 111   2  55   0   6   0]\n",
            " [ 35   8   9 922  31   0  10   0   4   0]\n",
            " [  5   0  89  23 810   1  41   0   5   0]\n",
            " [  0   0   0   2   0 951   1  22   3  10]\n",
            " [125   4 108  33  77   0 660   0  13   1]\n",
            " [  0   0   0   0   0  34   0 944   3  41]\n",
            " [  3   2   6   5   1   1   9   3 957   3]\n",
            " [  0   0   0   0   0  11   0  26   1 962]]\n",
            "Precisión: 0.8850269210698065\n",
            "F1 Score: 0.8840883434615058\n",
            "Accuracy: 0.8852\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. CLASIFICADOR TIPO AUTOENCODER VARIACIONAL"
      ],
      "metadata": {
        "id": "2uvDLKguZEh9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Input, Dense, Flatten, Reshape, Lambda, Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.losses import binary_crossentropy\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.layers import UpSampling2D\n",
        "\n",
        "# 1. Preparar los datos\n",
        "X_train = np.reshape(X_train, (len(X_train), 28, 28, 1))\n",
        "Z=X_train[::50].copy()\n",
        "Y=y_train[::50].copy()\n",
        "\n",
        "# Convertir las etiquetas a formato categórico\n",
        "y_train = to_categorical(y_train, 10)\n",
        "Y= to_categorical(Y, 10)\n",
        "\n",
        "# 2. Definir el encoder\n",
        "latent_dim = 2  # Dimensionalidad del espacio latente\n",
        "\n",
        "inputs = Input(shape=(28, 28, 1)) #Tamaño de las imágenes\n",
        "\n",
        "# Encoder\n",
        "encoder_conv1 = Conv2D(32, kernel_size=3, activation='relu', padding='same')(inputs)\n",
        "encoder_pool1 = MaxPooling2D((2, 2), padding='same')(encoder_conv1)\n",
        "encoder_conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(encoder_pool1)\n",
        "encoder_pool2 = MaxPooling2D((2, 2), padding='same')(encoder_conv2)\n",
        "\n",
        "# Flatten y capas densas para obtener z_mean y z_log_var\n",
        "x = Flatten()(encoder_pool2)\n",
        "x = Dense(512, activation='relu')(x)\n",
        "z_mean = Dense(latent_dim)(x)\n",
        "z_log_var = Dense(latent_dim)(x)\n",
        "\n",
        "# Muestreo\n",
        "def sampling(args):\n",
        "    z_mean, z_log_var = args\n",
        "    batch = K.shape(z_mean)[0]\n",
        "    dim = K.int_shape(z_mean)[1]\n",
        "    epsilon = K.random_normal(shape=(batch, dim))\n",
        "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
        "\n",
        "# Clasificación usando el espacio latente\n",
        "class_output = Dense(10, activation='softmax')(z)\n",
        "\n",
        "# Encoder Model\n",
        "encoder = Model(inputs, [z_mean, z_log_var, z, class_output], name='encoder')\n",
        "\n",
        "# Decoder Model\n",
        "latent_inputs = Input(shape=(latent_dim,))\n",
        "x = Dense(7 * 7 * 64, activation='relu')(latent_inputs)\n",
        "x = Reshape((7, 7, 64))(x)\n",
        "x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
        "x = UpSampling2D((2, 2))(x)  # Upsample to 14x14\n",
        "x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
        "x = UpSampling2D((2, 2))(x)  # Upsample to 28x28\n",
        "decoder_outputs = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
        "\n",
        "# Decoder Model\n",
        "decoder = Model(latent_inputs, decoder_outputs, name='decoder')\n",
        "\n",
        "# 4. Construir el VAE\n",
        "vae_outputs = decoder(encoder(inputs)[2])\n",
        "vae = Model(inputs, vae_outputs, name='vae')\n",
        "\n",
        "# 5. Definir la función de pérdida del VAE\n",
        "def vae_loss(inputs, outputs, z_mean, z_log_var):\n",
        "    xent_loss = binary_crossentropy(K.flatten(inputs), K.flatten(outputs))\n",
        "    kl_loss = -0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
        "    return K.mean(xent_loss + kl_loss)\n",
        "\n",
        "# Función de pérdida personalizada para el modelo combinado\n",
        "def combined_loss(inputs, outputs):\n",
        "    z_mean, z_log_var, z = encoder(inputs)\n",
        "    reconstruction_loss = vae_loss(inputs, outputs, z_mean, z_log_var)\n",
        "    return reconstruction_loss\n",
        "\n",
        "# Crear el modelo combinado\n",
        "combined_outputs = [vae_outputs, class_output]\n",
        "combined_model = Model(inputs, combined_outputs)\n",
        "\n",
        "# Compilar el modelo combinado\n",
        "combined_model.compile(optimizer='adam',\n",
        "                       loss=['binary_crossentropy', 'categorical_crossentropy'],\n",
        "                       loss_weights=[0.5, 0.5])\n",
        "\n",
        "# Entrenar el modelo\n",
        "history = combined_model.fit(Z, [Z, Y], epochs=10, batch_size=64)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Of2sqjt3ZIeG",
        "outputId": "89303214-afc4-4fd4-d1b4-c0f828b99a01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 789ms/step - loss: 1.5912\n",
            "Epoch 2/10\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 460ms/step - loss: 1.4469\n",
            "Epoch 3/10\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 643ms/step - loss: 1.3198\n",
            "Epoch 4/10\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 230ms/step - loss: 1.2726\n",
            "Epoch 5/10\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 363ms/step - loss: 1.1971\n",
            "Epoch 6/10\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 376ms/step - loss: 1.1280\n",
            "Epoch 7/10\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 453ms/step - loss: 1.0669\n",
            "Epoch 8/10\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 474ms/step - loss: 1.0223\n",
            "Epoch 9/10\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 629ms/step - loss: 0.9735\n",
            "Epoch 10/10\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 449ms/step - loss: 0.9452\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reconstrucción y clasificación\n",
        "predictions = combined_model.predict(X_train)\n",
        "reconstructed_images = predictions[0]\n",
        "predicted_classes = np.argmax(predictions[1], axis=1)\n",
        "y_test_labels = np.argmax(y_train, axis=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LI7mkp7wxNQx",
        "outputId": "8d5cfd56-8867-4986-ddbf-bd86c17ca92f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 83ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "y_true = y_test_labels\n",
        "y_pred = predicted_classes\n",
        "\n",
        "#Calcular accuracy (porcentaje total de aciertos)\n",
        "accuracy = np.mean(y_true == y_pred)\n",
        "\n",
        "# Calcular precisión (mide la exactitud de las predicciones positivas de un modelo)\n",
        "precision = precision_score(y_true, y_pred, average='weighted')\n",
        "\n",
        "# Calcular recall (mide la capacidad de un modelo para identificar todos los casos positivos)\n",
        "recall = recall_score(y_true, y_pred, average='weighted')\n",
        "\n",
        "# Calcular F1 score (combina la precisión y el recall en una sola medida.)\n",
        "f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "\n",
        "#Calcular matriz de confusion\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "# Imprimir resultados\n",
        "print(cm)\n",
        "print(\"Precisión:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A99Bw93jzSjR",
        "outputId": "ced00727-13ae-41ba-b75e-eeb582593d52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[379   9  11 433  13   2   1  12   0  82]\n",
            " [ 29 925  13   6  52   0   0   0   0   2]\n",
            " [168 267 109  52 322   9   8  25   0  56]\n",
            " [153  44   8 477  39   0   1   2   0 295]\n",
            " [261 171  25  94 371   5   5   4   0  38]\n",
            " [  1   0  16   0   0 216  13 723   0  20]\n",
            " [297 103  71 203 150   8  13  41   0 135]\n",
            " [  0   0   0   0   0   7  32 959   0  24]\n",
            " [ 21   8  48  18  10  77  55 555   0 198]\n",
            " [  0   2   3   0   0   2  97 138   0 758]]\n",
            "Precisión: 0.35990227835818295\n",
            "Recall: 0.4207\n",
            "F1 Score: 0.3515503799777518\n",
            "Accuracy: 0.4207\n"
          ]
        }
      ]
    }
  ]
}